{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik-palaniappan/Mini-Project-Speech-Emotion-Classification/blob/main/T7_Copy_of_M2_Supplementary_Audio_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "758ce2d3"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Supplementary: Audio feature extraction"
      ],
      "id": "758ce2d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdrEjSyJIfIX"
      },
      "source": [
        "**Note**: This notebook is an ungraded, supplementary notebook, provided as pre-reading for the 'Speech Emotion Recognition' Mini-project. This will help you to understand the various audio features and get introduced to the tools and packages used in audio feature extraction."
      ],
      "id": "rdrEjSyJIfIX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b3c3ed4"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "5b3c3ed4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc060162"
      },
      "source": [
        "In this supplementary notebook, you will be able to:\n",
        "\n",
        "* record audio samples and save it\n",
        "* load the audio data from the dataset\n",
        "* extract the features of the audio data"
      ],
      "id": "cc060162"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgBJOsaJM7jD"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/) database was recorded from four native English male speakers (identified as DC, JE, JK, KL), postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. It contains 480 British English utterances in different emotions. Emotion has been described psychologically in discrete categories: anger, disgust, fear, happiness, sadness and surprise.'Neutral' emotion was added to provide recordings of 7 emotion categories. The text material consisted of 15 sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. The 3 common and 2 Ã— 6 = 12 emotion-specific sentences were recorded as neutral to give 30 neutral sentences.\n",
        "\n",
        "  - This resulted in a total of 120 utterances per speaker, for example:\n",
        "\n",
        "      - Common: She had your dark suit in greasy wash water all year.\n",
        "      - Anger: Who authorized the unlimited expense account?\n",
        "      - Disgust: Please take this dirty table cloth to the cleaners for me.\n",
        "      - Fear: Call an ambulance for medical assistance.\n",
        "      - Happiness: Those musicians harmonize marvelously.\n",
        "      - Sadness: The prospect of cutting back spending is an unpleasant one for any governor.\n",
        "      - Surprise: The carpet cleaners shampooed our oriental rug.\n",
        "      - Neutral: The best way to learn is to solve extra problems."
      ],
      "id": "dgBJOsaJM7jD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71a2364d"
      },
      "source": [
        "### Download the dataset"
      ],
      "id": "71a2364d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efae21b9"
      },
      "source": [
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/SaveeData.zip\n",
        "!unzip -qq SaveeData.zip"
      ],
      "id": "efae21b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1384981c"
      },
      "source": [
        "# Install packages\n",
        "!pip -qq install librosa soundfile"
      ],
      "id": "1384981c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "259dde4b"
      },
      "source": [
        "### Import required packages"
      ],
      "id": "259dde4b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d34714b"
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile\n",
        "import re\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from datetime import datetime\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode"
      ],
      "id": "8d34714b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYnNYgL_xYdS"
      },
      "source": [
        "### Record Audio sample\n",
        "\n",
        "We will use the Javascript from IPython that will enable us to record the audio.\n",
        "\n",
        "* Create a directory to store all the recordings\n",
        "* Naming convention of the audio sample is `Audio_DateTime_label.wav`, For eg, *Audio_2021-06-20_11-30-30_angry.wav*\n",
        "* Upon calling the function `record()`, it prompts to enter the emotion (label) to name the file.\n",
        "\n",
        "While recording the audio, the utterance can be \"*Say the word boat*\" and you can show variation in the emotion"
      ],
      "id": "IYnNYgL_xYdS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pimvcJ7Py1oS"
      },
      "source": [
        "# Create a directory\n",
        "os.mkdir(\"Records\")"
      ],
      "id": "pimvcJ7Py1oS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10adc3c1"
      },
      "source": [
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=3):\n",
        "    emt = input(\"Enter emotion(label) to save the recording: \")\n",
        "    print(\"Start speaking!\")\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    display(Javascript(RECORD))\n",
        "    s = output.eval_js('record(%d)' % (sec*1000))\n",
        "    b = b64decode(s.split(',')[1])\n",
        "    with open('Records/audio_'+current_time+\"_\"+emt+'.wav','wb') as f:\n",
        "      f.write(b)\n",
        "    return 'Records/audio_'+current_time+\"_\"+emt+'.wav'"
      ],
      "id": "10adc3c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKNZPKW3zA_U"
      },
      "source": [
        "# call the function to record a sample\n",
        "record()"
      ],
      "id": "kKNZPKW3zA_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c92da7b8"
      },
      "source": [
        "# change the path and listen to recorded audio sample\n",
        "#ipd.Audio(\"SaveeData/JE/a08.wav\")"
      ],
      "id": "c92da7b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccda0e02"
      },
      "source": [
        "### Load the dataset"
      ],
      "id": "ccda0e02"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cb3c5c2"
      },
      "source": [
        "# The glob module is used to retrieve files/pathnames matching a specified pattern.\n",
        "# It is faster than other methods to match pathnames in directories\n",
        "wav_files = glob.glob(\"SaveeData/*/*.wav\")\n",
        "len(wav_files)"
      ],
      "id": "6cb3c5c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "312d38d4"
      },
      "source": [
        "### Explore the data and visualize"
      ],
      "id": "312d38d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b969945e"
      },
      "source": [
        "#### Various emotions in given dataset"
      ],
      "id": "b969945e"
    },
    {
      "cell_type": "code",
      "source": [
        "wav_files[0]"
      ],
      "metadata": {
        "id": "AY-tOVs2vRDS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AY-tOVs2vRDS"
    },
    {
      "cell_type": "code",
      "source": [
        "emotions_ = []\n",
        "mapping = {'a': 'angry', 'f': 'fear', 'd': 'disgust', 'h': 'happiness', 'sa': 'sadness', 'su': 'surprise', 'n': 'neutral'}\n",
        "for file_name in wav_files:\n",
        "    emotions_.append(mapping[re.sub('[0-9]*', '', file_name.split(\"/\")[-1].split(\".\")[0])])\n",
        "set(emotions_)"
      ],
      "metadata": {
        "id": "NIEFQXfJbRUB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NIEFQXfJbRUB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5Y2ErqJLca"
      },
      "source": [
        "####Speech/Audio analysis Python Package: **Librosa**\n",
        "\n",
        "[Librosa](https://librosa.org/doc/latest/index.html) is a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions and helps in audio feature extraction and manipulation, segmentation, visualization, and display of feature representations."
      ],
      "id": "6e5Y2ErqJLca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c40a63"
      },
      "source": [
        "#### Visualize sample audio signal using librosa"
      ],
      "id": "a6c40a63"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "771b70e0"
      },
      "source": [
        "sample_audio_path = '/content/SaveeData/DC/a03.wav'\n",
        "\n",
        "# librosa is used for analyzing and extracting features of an audio signal\n",
        "data, sampling_rate = librosa.load(sample_audio_path)\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# librosa.display.waveshow is used to plot waveform of amplitude vs time\n",
        "librosa.display.waveshow(data, sr=sampling_rate)\n",
        "plt.show()"
      ],
      "id": "771b70e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LRVezSINn8X"
      },
      "source": [
        "### Audio Features\n",
        "\n"
      ],
      "id": "5LRVezSINn8X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceaa5afb"
      },
      "source": [
        "### Extract features\n",
        "\n",
        "* Load an audio using `librosa.load()`\n",
        "\n",
        "    `load()` function loads an audio file and decodes it into a 1-dimensional array which is a time series x , and sr is a sampling rate of x . Default sr is 22kHz.\n",
        "\n",
        "\n",
        "* Apply short term Fourier transform using `librosa.stft()\n",
        "\n",
        "    `stft()` converts data into short term Fourier transform. STFT converts signal such that we can know the amplitude of given frequency at a given time. Using STFT we can determine the amplitude of various frequencies playing at a given time of an audio signal.\n",
        "\n",
        "\n",
        "* Apply Mel-Frequency Cepstral Coefficients (MFCC) using librosa.feature.mfcc\n",
        "\n",
        "    This feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. The mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10â€“20) which concisely describe the overall shape of a spectral envelope.\n",
        "    \n",
        "\n",
        "* Compute a chromagram from a waveform or power spectrogram using librosa.feature.chroma_stft\n",
        "    \n",
        "    Chroma features are a powerful tool for analyzing audio whose pitches can be meaningfully categorized (often into twelve categories) and whose tuning approximates to the equal-tempered scale. One main property of Chroma features is that they capture harmonic and melodic characteristics of music, while being robust to changes in timbre and instrumentation.\n",
        "\n",
        "\n",
        "* Compute a Mel-scaled spectrogram using librosa.feature.melspectrogram\n",
        "    \n",
        "    A Mel spectrogram is a spectrogram where the frequencies are converted to the Mel scale. Mel scale is a scale of pitches judged by listeners to be equal in distance one from another.\n",
        "\n",
        "For more information related to audio based features, refer [here](https://heartbeat.fritz.ai/working-with-audio-signals-in-python-6c2bd63b2daf)."
      ],
      "id": "ceaa5afb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e594300b"
      },
      "source": [
        "def extract_feature(file_name):\n",
        "    # load audio\n",
        "    X, sample_rate = librosa.load(file_name)\n",
        "    # apply stft()\n",
        "    stft=np.abs(librosa.stft(X))\n",
        "    result=np.array([])\n",
        "    # compute mfcc\n",
        "    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
        "    # combine the features\n",
        "    result=np.hstack((result, mfccs))\n",
        "    # compute chroma features and combine\n",
        "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "    result=np.hstack((result, chroma))\n",
        "    # compute melspectrogram and combine\n",
        "    mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
        "    result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "id": "e594300b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07fbc13e"
      },
      "source": [
        "sample_feature = extract_feature(wav_files[0]) #,mfcc=True, chroma=True, mel=True)\n",
        "sample_feature.shape"
      ],
      "id": "07fbc13e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbf9fb73"
      },
      "source": [
        "type(sample_feature)"
      ],
      "id": "cbf9fb73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWuiYSRZujrV"
      },
      "source": [
        "### Extract features and labels of all audio samples\n",
        "\n",
        "'DC', 'JE', 'JK' and 'KL' are four male speakers recorded for the SAVEE database. Audio files consist of audio WAV files..\n",
        "\n",
        "There are 15 sentences for each of the 7 emotion categories.\n",
        "The initial letter(s) of the file name represents the emotion class, and the following digits represent the sentence number.\n",
        "The letters 'a', 'd', 'f', 'h', 'n', 'sa' and 'su' represent 'anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness' and 'surprise' emotion classes respectively.\n",
        "\n",
        "E.g., '**SaveeData/JK/d03.wav**' is the 3rd disgust sentence.\n"
      ],
      "id": "BWuiYSRZujrV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LszCs0X2ujrX"
      },
      "source": [
        "emotions_savee = {\"a\":0, \"d\":1, \"f\":2, \"h\":3, \"n\":4, \"sa\":5, \"su\":6}\n",
        "emotions_savee"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "LszCs0X2ujrX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpwttiUHujrY"
      },
      "source": [
        "# Declare list to append features and labels\n",
        "features, labels = [], []\n",
        "\n",
        "for wv_file in wav_files:\n",
        "  features.append(extract_feature(wv_file))\n",
        "  # extracting label\n",
        "  emt = re.sub('[0-9]*', '', wv_file.split(\"/\")[-1].split(\".\")[0])\n",
        "  labels.append(emotions_savee[emt])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "MpwttiUHujrY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CupUb6NJujrY"
      },
      "source": [
        "len(features), len(labels)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "CupUb6NJujrY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvFLfNruujrY"
      },
      "source": [
        "print(set(labels))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "IvFLfNruujrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaquN_fSujrZ"
      },
      "source": [
        "For further reading on audio acquisition, representation and storage refer the [book](http://www.dcs.gla.ac.uk/~vincia/textbook.pdf): *Camastra and Vinciarelli, 2007, Machine Learning for Audio, Image and Video Analysis*"
      ],
      "id": "zaquN_fSujrZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Grm4xrtZ5q0H"
      },
      "id": "Grm4xrtZ5q0H",
      "execution_count": null,
      "outputs": []
    }
  ]
}